{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1053f53",
   "metadata": {},
   "source": [
    "# Best Practices, Performance, & Troubleshooting\n",
    "\n",
    "This chapter provides tips for working effectively with `Flow`, optimizing performance, and avoiding common mistakes. `Flow` is built to be flexible and safe by default, but understanding its lazy behavior and internal memory handling can help you write cleaner and more efficient pipelines.\n",
    "\n",
    "## You Can Reuse Any Flow\n",
    "\n",
    "Unlike regular Python iterators, a `Flow` can be reused as many times as you like. You can:\n",
    "\n",
    "- Call `.collect()` multiple times\n",
    "- Loop through it more than once\n",
    "- Peek at `.first()` or `.keys()` safely\n",
    "- Chain further operations even after materialization\n",
    "\n",
    "```python\n",
    "flow.first()   # buffers 1 record\n",
    "flow.head(100) # buffers another 100\n",
    "flow.collect() # buffers everything\n",
    "```\n",
    "\n",
    "If you're reusing the same Flow repeatedly (especially on large datasets), this memory can add up.\n",
    "\n",
    "## Use `.cache()` for Predictability\n",
    "\n",
    "To avoid memory surprises and improve performance when reusing data, use `.cache()`:\n",
    "\n",
    "```python\n",
    "flow = Flow.from_jsonl(\"match_events.jsonl\").cache()\n",
    "```\n",
    "\n",
    "This:\n",
    "\n",
    "- Loads everything once\n",
    "- Stores it in memory explicitly\n",
    "- Makes all operations faster and repeatable\n",
    "\n",
    "Use `.cache()` if:\n",
    "\n",
    "- You're branching into multiple sub-pipelines\n",
    "- You're debugging\n",
    "- You want to avoid surprises from internal buffering\n",
    "\n",
    "This caches the entire `Flow` in memory, making it reusable and predictable. It's like a \"save-point\" partway through your pipeline so you can reuse it without having to run the previous steps again.\n",
    "\n",
    "## Know Which Operations Are Memory-Heavy\n",
    "\n",
    "The following operations materialize the full `Flow` into memory:\n",
    "\n",
    "- `.collect()`\n",
    "- `.to_pandas()` / `.to_json()` / `.to_jsonl()`\n",
    "- `.group_by()` and `.summary()`\n",
    "- `.sort()`, `.row_number()`, `.sample()`, `.take_last()`\n",
    "- `.last()`\n",
    "- `.len()`\n",
    "\n",
    "## Recommended Patterns\n",
    "\n",
    "\n",
    "| Situation                      | Recommendation                               |\n",
    "|-------------------------------|----------------------------------------------|\n",
    "| One-off pipeline              | Just use `.filter(...).assign(...).collect()`|\n",
    "| Reuse across multiple steps   | Use `.cache()` once early                    |\n",
    "| Working with large files      | Use `.from_jsonl()` + streaming methods      |\n",
    "| Schema exploration            | Use `.first()`, `.keys(limit=10)`           |\n",
    "| Testing intermediate steps    | Use `.limit()`, `.head()`, `.pipe()`        |\n",
    "| Avoiding memory surprises     | Use `.cache()` before `.collect()` or `.sort()` |\n",
    "\n",
    "## Debugging Pipelines\n",
    "\n",
    "### Break Long Chains\n",
    "\n",
    "Split your pipeline into steps:\n",
    "\n",
    "```python\n",
    "step1 = flow.filter(...)\n",
    "print(step1.head().collect())\n",
    "step2 = step1.assign(...)\n",
    "```\n",
    "\n",
    "### Use `.pipe()` to Inject Debug Prints\n",
    "\n",
    "```python\n",
    "def peek(flow):\n",
    "    print(flow.head(3).collect())\n",
    "    return flow\n",
    "\n",
    "flow.filter(...).pipe(peek).assign(...)\n",
    "```\n",
    "\n",
    "## When Not to Use Flow\n",
    "\n",
    "`Flow` is ideal for working with JSON-like records in Python. But other tools may be better if:\n",
    "\n",
    "- You need SQL joins across many large tables\n",
    "- You want indexed storage or transactional queries\n",
    "- Your dataset is too large to sort or group in memory\n",
    "- You need true real-time stream processing \n",
    "\n",
    "## Summary\n",
    "\n",
    "`Flow` is designed to be safe, reusable, and lazy by default. You can:\n",
    "\n",
    "- Chain and reuse pipelines without worrying about exhaustion\n",
    "- Safely explore and inspect data at any point\n",
    "- Cache data for repeatable performance\n",
    "\n",
    "Understanding how `Flow` balances laziness with internal buffering will help you stay efficient and in control when processing football (or any) event data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
