{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe42299",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Matchflow: a lazy, streaming Data‐Pipeline\n",
    "\n",
    "In football analytics, much of the data - events, matches, player tracking - is delivered as nested JSON records. These files can be large, noisy, and full of nested structure, but we often want to perform familiar operations: filtering, selecting fields, computing aggregates, or joining on player or match IDs.\n",
    "\n",
    "\n",
    "In many tasks, you want to run a **lightweight data pipeline** over streams of JSON/dict records without loading everything into memory.\n",
    "\n",
    "That’s where **Flow** comes in.\n",
    "\n",
    "## Understanding How `Flow` Works\n",
    "\n",
    "Think of a `Flow` as a smart, efficient assembly line for your data records. You start with a stream of raw data (like a list of game events from a JSON file), and then you tell `Flow` what steps to perform on each record as it moves along the line.\n",
    "\n",
    "### Planning Your Work (Lazy Evaluation & Chaining)\n",
    "\n",
    "When you use methods like `.filter(...)`, `.select(...),` or `.assign(...)` on a `Flow`, you're not immediately processing all your data. Instead, you're building a plan or a recipe. Each step you add creates a new `Flow` object that remembers all the previous steps plus the new one you just added. This is called \"chaining.\"\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# Let's say raw_events is a list of dictionaries from a StatsBomb file\n",
    "# This line just sets up the starting point\n",
    "data_plan = Flow(raw_events)\n",
    "\n",
    "# This adds a \"filter\" step to the plan. No data is processed yet.\n",
    "filtered_plan = data_plan.filter(lambda event: event.get(\"type_name\") == \"Shot\")\n",
    "\n",
    "# This adds a \"select\" step. Still just a plan!\n",
    "final_plan = filtered_plan.select(\"player_name\", \"shot_outcome_name\")\n",
    "```\n",
    "\n",
    "At this stage, `final_plan` knows it needs to: 1) get raw_events, 2) filter for shots, 3) select player and outcome. But no actual filtering or selecting has happened. This is \"lazy evaluation\" – work is delayed until absolutely necessary.\n",
    "\n",
    "### Doing the Work (Materialization)\n",
    "\n",
    "The actual data processing – running records through your planned assembly line – happens only when you ask for the final results. This is often called \"materialization.\"\n",
    "\n",
    "Common ways to get results:\n",
    "\n",
    "- `.collect()`: Turns the final planned output into a Python list of dictionaries.\n",
    "- `.to_pandas()`: Converts the output into a pandas DataFrame.\n",
    "- `.to_jsonl(\"output.jsonl\")`: Writes each resulting record to a file.\n",
    "- Looping: `for record in final_plan: print(record)`\n",
    "\n",
    "**Example (continuing from above):**\n",
    "\n",
    "```python\n",
    "shot_data_list = final_plan.collect()\n",
    "# NOW the work happens:\n",
    "# 1. Records from raw_events are read one by one.\n",
    "# 2. Each record is checked if it's a \"Shot\".\n",
    "# 3. For shots, only \"player_name\" and \"shot_outcome_name\" are kept.\n",
    "# 4. These processed records are collected into shot_data_list.\n",
    "```\n",
    "\n",
    "###  Important: The \"One-Way Street\" Nature (How `Flow` works):\n",
    "\n",
    "`Flow` is designed to be efficient with potentially large streams of data. It generally processes data in a single pass. When you use an operation like `flow.limit(5)`, it doesn't just give you the first 5 items and leave the original flow untouched at the beginning.\n",
    "\n",
    "- Internally, Flow uses a clever trick (`itertools.tee`) that's like putting a Y-splitter in a pipe.\n",
    "- When you do `limited_flow = original_flow.limit(5)`:\n",
    "    - `limited_flow` gets one branch of the \"pipe\" that will provide the first 5 items.\n",
    "    - `original_flow` updates itself to use the other branch, which will effectively start after those first 5 items if `limited_flow` is actually used (e.g., by calling `.collect()` on it).\n",
    "- **Think of it like this:** If you have a stack of 100 pages and you say `my_flow.limit(10).get_pages()`, you get the first 10 pages. The `my_flow` stack itself now effectively starts at page 11 for any subsequent operations on `my_flow`.\n",
    "- **What this means for you:** If you want to perform two completely independent analyses starting from the very beginning of your original data, you should create a fresh `Flow` from your source data for each analysis:\n",
    "\n",
    "```python\n",
    "# Good: Two independent analyses\n",
    "analysis1_result = Flow(raw_events).filter(some_condition).collect()\n",
    "analysis2_result = Flow(raw_events).limit(10).collect() # Starts fresh from raw_events\n",
    "\n",
    "# Potentially confusing: Second operation acts on a progressed Flow\n",
    "# original_flow = Flow(raw_events)\n",
    "# analysis1_result = original_flow.filter(some_condition).collect()\n",
    "# # If analysis1 consumed many items, analysis2 might get fewer than 10 or different items!\n",
    "# analysis2_result = original_flow.limit(10).collect()\n",
    "```\n",
    "\n",
    "### Strengths of this Approach\n",
    "\n",
    "- Memory Efficient: Excellent for large data files (like detailed event data) because it processes records one by one (or in small chunks) instead of loading everything into memory at once.\n",
    "- Clear & Readable Pipelines: Chaining operations `(.filter().select().assign()...)` makes your data transformation logic easy to follow.\n",
    "- Flexible: You can combine many different operations to clean, reshape, and analyze your data.\n",
    "- Lazy: If you only need the first few records (e.g., `flow.head(5).collect()`) or to check if data exists (`flow.first()`), it avoids processing the entire dataset.\n",
    "\n",
    "### Limitations & Things to Keep in Mind\n",
    "\n",
    "- Single-Pass for a Given Flow Instance: As explained above, once part of a `Flow` is consumed by an operation that returns a result (or by iterating it), that `Flow` instance has \"moved forward.\" For truly independent operations on the full original dataset, start a new `Flow` from your source.\n",
    "- Some Operations Consume Everything: Certain methods need to see all the data to work. For example, `.sort()`, `.summary()`, .`last()`, or getting the `len(flow)`. After these, the `Flow` instance you called them on will be \"exhausted\" (its internal iterator will be empty). The methods themselves will return a new `Flow` (or data structure) with the result.\n",
    "- Not a Database Replacement: While powerful for data transformations, it's not designed for complex relational queries, persistent storage, or indexed lookups on massive datasets where a proper database would be more suitable.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
