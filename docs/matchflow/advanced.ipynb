{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ad761c",
   "metadata": {},
   "source": [
    "# Advanced Pipeline Operations\n",
    "\n",
    "Beyond transforming individual records and basic grouping, `Flow` offers a suite of more advanced methods for manipulating and combining data streams. These operations can help you with tasks like sorting, joining different datasets, handling duplicates, and working with sequences of records.\n",
    "\n",
    "Many of these methods, especially those that need to reorder or compare records across the stream (like `sort` or `join`), may need to materialize part or all of the stream into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def57a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from penaltyblog.matchflow import Flow\n",
    "from pprint import pprint\n",
    "\n",
    "matches = [\n",
    "    {\n",
    "        \"match_id\": 123,\n",
    "        \"competition_name\": \"Premier League\",\n",
    "        \"home_team\": \"Manchester City\",\n",
    "        \"away_team\": \"Arsenal\",\n",
    "        \"match_date\": \"2023-10-08\",\n",
    "    },\n",
    "    {\n",
    "        \"match_id\": 456,\n",
    "        \"competition_name\": \"Champions League\",\n",
    "        \"home_team\": \"Real Madrid\",\n",
    "        \"away_team\": \"Bayern Munich\",\n",
    "        \"match_date\": \"2023-10-10\",\n",
    "    },\n",
    "]\n",
    "\n",
    "events = [\n",
    "    {\n",
    "        \"event_id\": 1,\n",
    "        \"match_id\": 123,\n",
    "        \"period\": 1,\n",
    "        \"timestamp\": \"00:01:30.500\",\n",
    "        \"type_name\": \"Pass\",\n",
    "        \"player_name\": \"Kevin De Bruyne\",\n",
    "        \"location\": [60.1, 40.3],\n",
    "        \"pass_recipient_name\": \"Erling Haaland\",\n",
    "        \"pass_outcome_name\": \"Complete\",\n",
    "        \"team_name\": \"Manchester City\",\n",
    "    },\n",
    "    {\n",
    "        \"event_id\": 2,\n",
    "        \"match_id\": 123,\n",
    "        \"period\": 1,\n",
    "        \"timestamp\": \"00:01:32.100\",\n",
    "        \"type_name\": \"Shot\",\n",
    "        \"player_name\": \"Erling Haaland\",\n",
    "        \"location\": [85.5, 50.2],\n",
    "        \"shot_xg\": 0.05,\n",
    "        \"shot_outcome_name\": \"Goal\",\n",
    "        \"team_name\": \"Manchester City\",\n",
    "    },\n",
    "    {\n",
    "        \"event_id\": 3,\n",
    "        \"match_id\": 123,\n",
    "        \"period\": 1,\n",
    "        \"timestamp\": \"00:02:05.000\",\n",
    "        \"type_name\": \"Duel\",\n",
    "        \"player_name\": \"Rodri\",\n",
    "        \"duel_type_name\": \"Tackle\",\n",
    "        \"duel_outcome_name\": \"Won\",\n",
    "        \"team_name\": \"Manchester City\",\n",
    "    },\n",
    "    {\n",
    "        \"event_id\": 4,\n",
    "        \"match_id\": 123,\n",
    "        \"period\": 1,\n",
    "        \"timestamp\": \"00:02:10.000\",\n",
    "        \"type_name\": \"Pass\",\n",
    "        \"player_name\": \"Kevin De Bruyne\",\n",
    "        \"location\": [70.0, 25.0],\n",
    "        \"pass_recipient_name\": \"Jack Grealish\",\n",
    "        \"pass_outcome_name\": \"Incomplete\",\n",
    "        \"team_name\": \"Manchester City\",\n",
    "    },\n",
    "    {\n",
    "        \"event_id\": 5,\n",
    "        \"match_id\": 123,\n",
    "        \"period\": 1,\n",
    "        \"timestamp\": \"00:03:00.000\",\n",
    "        \"type_name\": \"Shot\",\n",
    "        \"player_name\": \"Bukayo Saka\",\n",
    "        \"location\": [75.0, 60.0],\n",
    "        \"shot_xg\": 0.01,\n",
    "        \"shot_outcome_name\": \"Post\",\n",
    "        \"team_name\": \"Arsenal\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee08f3f",
   "metadata": {},
   "source": [
    "## Sorting and Ordering\n",
    "\n",
    "### Sorting Records (`.sort()`)\n",
    "The `.sort()` method sorts the records in the `Flow` based on the values in a specified field.\n",
    "\n",
    "Important: Sorting requires all records to be loaded into memory to determine their correct order. This means `.sort()` materializes and consumes the `Flow` it's called on, returning a new `Flow` with the sorted records.\n",
    "\n",
    "Records where the sort key (`by` field) is `None` are always placed at the very end of the sorted sequence, regardless of the reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59704ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'event_id': 2,\n",
       " 'match_id': 123,\n",
       " 'period': 1,\n",
       " 'timestamp': '00:01:32.100',\n",
       " 'type_name': 'Shot',\n",
       " 'player_name': 'Erling Haaland',\n",
       " 'location': [85.5, 50.2],\n",
       " 'shot_xg': 0.05,\n",
       " 'shot_outcome_name': 'Goal',\n",
       " 'team_name': 'Manchester City'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will sort events chronologically.\n",
    "sorted_events_flow = Flow(events).sort(by=\"timestamp\")\n",
    "\n",
    "# Example: Sorting shots by xG in descending order\n",
    "shots_flow = (\n",
    "    Flow(events)\n",
    "    .filter(lambda r: r.get(\"type_name\") == \"Shot\")\n",
    "    .sort(by=\"shot_xg\", reverse=True)\n",
    ")\n",
    "\n",
    "# Get the shot with the highest xG\n",
    "shots_flow.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce970a3",
   "metadata": {},
   "source": [
    "### Assigning Row Numbers (`.row_number()`)\n",
    "\n",
    "The `.row_number()` method first sorts the records by the by field (similar to `.sort()`) and then assigns a sequential integer rank (starting from 1) to each record in a new field.\n",
    "\n",
    "Like `.sort()`, this method also materializes and consumes the Flow.\n",
    "\n",
    "Records where the `by` field is None are placed at the end and receive None as their row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffc3db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'event_id': 2,\n",
      "  'location': [85.5, 50.2],\n",
      "  'match_id': 123,\n",
      "  'period': 1,\n",
      "  'player_name': 'Erling Haaland',\n",
      "  'shot_outcome_name': 'Goal',\n",
      "  'shot_xg': 0.05,\n",
      "  'team_name': 'Manchester City',\n",
      "  'timestamp': '00:01:32.100',\n",
      "  'type_name': 'Shot',\n",
      "  'xg_rank': 1},\n",
      " {'event_id': 5,\n",
      "  'location': [75.0, 60.0],\n",
      "  'match_id': 123,\n",
      "  'period': 1,\n",
      "  'player_name': 'Bukayo Saka',\n",
      "  'shot_outcome_name': 'Post',\n",
      "  'shot_xg': 0.01,\n",
      "  'team_name': 'Arsenal',\n",
      "  'timestamp': '00:03:00.000',\n",
      "  'type_name': 'Shot',\n",
      "  'xg_rank': 2}]\n"
     ]
    }
   ],
   "source": [
    "shots_flow = (\n",
    "    Flow(events)\n",
    "    .filter(lambda r: r.get(\"type_name\") == \"Shot\")\n",
    "    .row_number(by=\"shot_xg\", new_field=\"xg_rank\", reverse=True)\n",
    ")\n",
    "\n",
    "# Each shot record now has an 'xg_rank' field.\n",
    "# The shot with the highest xG will have xg_rank = 1.\n",
    "pprint(shots_flow.head(2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2ed0d",
   "metadata": {},
   "source": [
    "## Slicing and Sampling the Stream\n",
    "\n",
    "We've seen `head()` (which is also an alias for `limit()`) already. Here's a recap and a method for taking from the end.\n",
    "\n",
    "### Limiting Records (`.limit()` / `.head()`)\n",
    "\n",
    "`limit(n)` returns a new `Flow` that will yield at most the first `n` records from the original `Flow`. The original `Flow`'s internal pointer advances if the limited `Flow` is consumed.\n",
    "\n",
    "```python\n",
    "first_five_events = events_flow.limit(5)\n",
    "# equivalent to:\n",
    "first_five_events = events_flow.head(5)\n",
    "```\n",
    "\n",
    "### Taking the Last Records (`.take_last()`)\n",
    "\n",
    "`take_last(n)` returns a new `Flow` containing the last `n` records.\n",
    "\n",
    "Important: This method needs to read the entire stream to determine which records are the last ones. It materializes and consumes the Flow it's called on.\n",
    "\n",
    "```python\n",
    "last_three_events_flow = events_flow.take_last(3)\n",
    "```\n",
    "\n",
    "### Reservoir Sampling (`.sample()`)\n",
    "\n",
    "`sample(n, seed=None)` performs reservoir sampling to select exactly `n` records uniformly at random from the stream.\n",
    "\n",
    "This method also materializes and consumes the `Flow` as it needs to process all records to build the sample reservoir.\n",
    "\n",
    "If the stream has fewer than `n` items, all items are returned.\n",
    "\n",
    "```python\n",
    "# set seed for reproducibility\n",
    "random_sample_10_events = events_flow.sample(n=10, seed=42) \n",
    "```\n",
    "\n",
    "### Bernoulli Sampling (`.sample_frac()`)\n",
    "\n",
    "`sample_frac(frac, seed=None)` includes each record in the output `Flow` with a probability of `frac` (a float between 0.0 and 1.0).\n",
    "\n",
    "This method processes the stream lazily and does not need to materialize everything.\n",
    "\n",
    "The number of records in the output is approximate.\n",
    "\n",
    "```python\n",
    "approx_20_percent_of_events = events_flow.sample_frac(frac=0.2, seed=123)\n",
    "```\n",
    "\n",
    "## Combining Flows (`.concat()`)\n",
    "\n",
    "The `concat()` method allows you to append one or more other `Flow` objects to the end of the current `Flow`, creating a single, longer `Flow`.\n",
    "\n",
    "```python\n",
    "match1_events_flow = Flow.statsbomb.from_github_file(match_id=123, type=\"events\")\n",
    "match2_events_flow = Flow.statsbomb.from_github_file(match_id=456, type=\"events\")\n",
    "\n",
    "combined_events_flow = match1_events_flow.concat(match2_events_flow)\n",
    "# Now combined_events_flow will stream all records from match1, followed by all from match2.\n",
    "```\n",
    "\n",
    "The original flows are effectively chained together; records are streamed lazily.\n",
    "\n",
    "## Joining Data (`.join()`)\n",
    "\n",
    "Joining is a fundamental operation for combining data from two different sources based on a common key. `my_flow.join(other_flow, left_on, right_on, fields=None, how=\"left\")` merges records from my_flow (the \"left\" side) with records from other_flow (the \"right\" side).\n",
    "\n",
    "- **Important**: The other_flow (or list of dicts) is fully materialized into memory to create an efficient lookup table for the join. The my_flow (left side) is streamed.\n",
    "- `left_on`: The field name in my_flow to join on.\n",
    "- `right_on`: The field name in other_flow to join on. If None, it defaults to the left_on field name.\n",
    "- `fields`: An optional list of field names to include from the other_flow's matching records. If `None`, all fields from other_flow (except the right_on key itself) are included.\n",
    "- how:\n",
    "    - \"left\" (default): Keeps all records from my_flow. If a match is found in other_flow, fields are merged. If no match, fields from other_flow are not added (effectively None).\n",
    "    - \"inner\": Only keeps records from my_flow where a match is found in other_flow.\n",
    "\n",
    "### Example: Adding competition name to our events data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_flow = Flow(events)\n",
    "matches_flow = Flow(matches)\n",
    "\n",
    "events_with_matchesflow = events_flow.join(\n",
    "    other=matches_flow,\n",
    "    left_on=\"match_id\",  # Key in events_flow\n",
    "    right_on=\"match_id\",  # Key in match_info_flow\n",
    "    fields=[\"competition_name\"],  # Only bring this field from match_info_flow\n",
    ")\n",
    "\n",
    "results = events_with_matchesflow.collect()\n",
    "\n",
    "pprint(results[0])\n",
    "\n",
    "# If a match_id in events_flow didn't exist in matches_flow,\n",
    "# 'competition_name' would be missing from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024e7f8",
   "metadata": {},
   "source": [
    "### Example: Inner join to get events only for matches we have the competition name for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b46193",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_flow = Flow(events)\n",
    "matches_flow = Flow(matches)\n",
    "\n",
    "events_with_matchesflow = events_flow.join(\n",
    "    other=matches_flow,\n",
    "    left_on=\"match_id\",  # Key in events_flow\n",
    "    right_on=\"match_id\",  # Key in match_info_flow\n",
    "    fields=[\"competition_name\"],  # Only bring this field from match_info_flow\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "results = events_with_matchesflow.collect()\n",
    "\n",
    "# This time, if a match_id in events_flow didn't exist in matches_flow,\n",
    "# that record would be dropped from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc3fe8",
   "metadata": {},
   "source": [
    "The `other` argument to `join` can also be a plain Python list of dictionaries instead of a `Flow` object.\n",
    "\n",
    "## Handling Duplicates\n",
    "\n",
    "### Dropping Duplicate Records (`.drop_duplicates()`)\n",
    "\n",
    "- `.drop_duplicates(*fields, keep=\"first\")` removes duplicate records.\n",
    "- If no `*fields` are specified, it considers the entire record for duplication.\n",
    "- If `*fields` are provided, only those fields are used to identify duplicates.\n",
    "- keep:\n",
    "    - \"first\" (default): Keeps the first occurrence of a duplicate set.\n",
    "    - \"last\": Keeps the last occurrence.\n",
    "    - False: Drops all records that are part of any duplicate set.\n",
    "\n",
    "This method generally needs to keep track of \"seen\" records/keys, so it can be memory-intensive depending on the number of unique keys and size of your data.\n",
    "\n",
    "### Example: Dropping exact duplicate events (if any)\n",
    "\n",
    "```python\n",
    "unique_events_flow = events_flow.drop_duplicates()\n",
    "\n",
    "first_event_by_player_type_flow = events_flow.drop_duplicates(\"player_name\", \"type_name\", keep=\"first\")\n",
    "```\n",
    "\n",
    "## Getting Unique Field Values or Combinations (`.unique()`)\n",
    "\n",
    "`.unique(*fields)` returns a `Flow` of unique values or combinations of values for the specified fields.\n",
    "\n",
    "If one field is given (e.g., `my_flow.unique(\"player_name\")`), the output `Flow` will contain records like `{\"player_name\": \"Player A\"}, {\"player_name\": \"Player B\"}` for each unique player.\n",
    "\n",
    "If multiple fields are given (e.g., `my_flow.unique(\"team_name\", \"type_name\")`), the output `Flow` will contain records representing unique combinations, like `{\"team_name\": \"Team A\", \"type_name\": \"Pass\"}, {\"team_name\": \"Team A\", \"type_name\": \"Shot\"}`.\n",
    "\n",
    "If no fields are specified, it behaves like `.drop_duplicates()` considering the whole record, yielding unique records.\n",
    "\n",
    "Similar to `drop_duplicates`, this can be memory-intensive depending on the number of unique keys present.\n",
    "\n",
    "### Example: Get a list of all unique player names\n",
    "\n",
    "```python\n",
    "unique_players_flow = events_flow.unique(\"player_name\")\n",
    "unique_player_names = [r[\"player_name\"] for r in unique_players_flow.collect()]\n",
    "```\n",
    "\n",
    "These advanced operations provide significant power for more complex data wrangling and analysis scenarios. The `join` method, in particular, is useful for enriching datasets by combining information from multiple sources. Always be mindful of which operations might consume the entire stream into memory, especially when working with very large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf82f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
