{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9a4616",
   "metadata": {},
   "source": [
    "# Working with Files: Input & Output\n",
    "\n",
    "A key part of any data workflow is loading data from various sources and saving your results. `Flow` provides convenient methods for working with common formats - especially JSON and JSON Lines.\n",
    "\n",
    "\n",
    "## Loading Data into Flow\n",
    "\n",
    "`Flow` offers class methods (which you call like `Flow.from_something(...)`) to create a `Flow` instance directly from data sources.\n",
    "\n",
    "### From In-Memory Python Objects (`.from_records()`)\n",
    "\n",
    "If you already have your data as a Python list of dictionaries (or just a single dictionary, or any iterable of dictionaries), you can easily create a `Flow`:\n",
    "\n",
    "```python\n",
    "# List of dictionaries\n",
    "my_data = [\n",
    "    {\"id\": 1, \"value\": \"A\"},\n",
    "    {\"id\": 2, \"value\": \"B\"}\n",
    "]\n",
    "flow_from_list = Flow.from_records(my_data)\n",
    "\n",
    "# Single dictionary\n",
    "single_record = {\"id\": 3, \"value\": \"C\"}\n",
    "flow_from_dict = Flow.from_records(single_record) # Creates a Flow with one record\n",
    "\n",
    "# From a generator function\n",
    "def my_generator():\n",
    "    for i in range(3):\n",
    "        yield {\"id\": i, \"generated_value\": i*10}\n",
    "\n",
    "flow_from_gen = Flow.from_records(my_generator())\n",
    "\n",
    "# You can also use the Flow constructor directly for iterables:\n",
    "flow_from_constructor = Flow(my_data)\n",
    "flow_from_constructor_gen = Flow(my_generator())\n",
    "```\n",
    "\n",
    "`.from_records()` is flexible and is often the starting point if your data is already in Python.\n",
    "\n",
    "### From JSON Lines Files (`.from_jsonl()`)\n",
    "\n",
    "[JSON Lines](https://jsonlines.org/) (`.jsonl`) is a convenient format where each line in the file is a separate, valid JSON object. This format is excellent for streaming large datasets.\n",
    "\n",
    "```python\n",
    "# Assuming 'match_events.jsonl' exists and each line is a JSON event dictionary\n",
    "events_flow = Flow.from_jsonl(\"path/to/your/match_events.jsonl\")\n",
    "\n",
    "# You can also specify encoding if it's not UTF-8\n",
    "events_flow_custom_encoding = Flow.from_jsonl(\"data.jsonl\", encoding=\"latin-1\")\n",
    "\n",
    "# Now you can chain operations:\n",
    "shots_flow = events_flow.filter(lambda r: r.get(\"type_name\") == \"Shot\")\n",
    "shots_data = shots_flow.collect()\n",
    "```\n",
    "\n",
    "`Flow` will read the file line by line, parsing each line as a JSON record and yielding it into the stream. This is memory-efficient as the whole file isn't loaded at once.\n",
    "\n",
    "### From a Single JSON File (`.from_file()`)\n",
    "\n",
    "If your data is in a single JSON file that contains either a JSON array (a list of objects) or a single JSON object, you can use \n",
    "`.from_file()`:\n",
    "\n",
    "```python\n",
    "# Case 1: File contains a JSON array: [{\"id\":1,...}, {\"id\":2,...}]\n",
    "flow_from_array_file = Flow.from_file(\"path/to/data_array.json\")\n",
    "\n",
    "# Case 2: File contains a single JSON object: {\"match_id\":123, \"data\":{...}}\n",
    "# This will result in a Flow with a single record.\n",
    "flow_from_object_file = Flow.from_file(\"path/to/single_object.json\")\n",
    "```\n",
    "\n",
    "Note: `.from_file()` reads the entire file content into memory to parse the JSON structure, so it's best for files that comfortably fit in memory. For very large arrays of JSON objects, `.from_jsonl()` is preferred if you can use that format.\n",
    "\n",
    "### From a Folder of JSON Files (`.from_folder()`)\n",
    "\n",
    "If you have multiple JSON files in a single folder, and each file contains either a JSON array of records or a single JSON record, \n",
    "`.from_folder()` can load them all into a single `Flow`.\n",
    "\n",
    "```python\n",
    "# Assuming 'data_folder/' contains 'file1.json', 'file2.json', etc.\n",
    "# Each .json file can be a list of records or a single record.\n",
    "combined_flow = Flow.from_folder(\"path/to/your/data_folder\")\n",
    "\n",
    "# Non-JSON files in the folder are skipped.\n",
    "# The records from all JSON files are streamed together.\n",
    "```\n",
    "\n",
    "This method iterates through the files in the folder, reads each one, and yields its records.\n",
    "\n",
    "### From a Glob Pattern (`.from_glob()`)\n",
    "\n",
    "For more flexible file matching, including searching in subdirectories, you can use `.from_glob()`:\n",
    "\n",
    "```python\n",
    "# Load all JSON files in 'data_folder' and its subfolders\n",
    "all_json_flow = Flow.from_glob(\"path/to/your/data_folder/**/*.json\")\n",
    "\n",
    "# Load all event files from a competition season\n",
    "season_events_flow = Flow.from_glob(\"competitions/super_league/season_2023/events_*.json\")\n",
    "```\n",
    "\n",
    "`.from_glob()` uses Python's glob module to find matching file paths and then processes each file similar to `.from_folder().`\n",
    "\n",
    "üí° Files that don't contain valid JSON or are not list/dict will be skipped silently.\n",
    "\n",
    "### Special Loaders: StatsBomb Open Data (`.statsbomb.from_github_file()`)\n",
    "\n",
    "`Flow` includes a convenient helper to directly load StatsBomb open data files. This loader supports the \"events\", \"lineups\", \"matches\" and similar types found in the StatsBomb Open Data [GitHub repo](https://github.com/statsbomb/open-data/).\n",
    "\n",
    "```python\n",
    "# Load event data for StatsBomb match_id 266516\n",
    "match_id = 266516\n",
    "events_flow = Flow.statsbomb.from_github_file(match_id=match_id, type=\"events\")\n",
    "\n",
    "# Load lineup data for the same match\n",
    "lineups_flow = Flow.statsbomb.from_github_file(match_id=match_id, type=\"lineups\")\n",
    "\n",
    "# Process as usual\n",
    "shots = events_flow.filter(lambda r: r.get(\"type_name\") == \"Shot\").collect()\n",
    "```\n",
    "\n",
    "This method handles fetching the data from the URL and parsing the JSON.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6605503",
   "metadata": {},
   "source": [
    "## Saving Data from a Flow\n",
    "\n",
    "Once you've transformed your data, you can save it using one of Flow‚Äôs output methods.\n",
    "\n",
    "üí° Output methods will materialize the `Flow`.\n",
    "\n",
    "### `.to_json_files()`: One File Per Record\n",
    "\n",
    "Writes each record to a separate .json file in a folder.\n",
    "\n",
    "```python\n",
    "# Create one file per record\n",
    "Flow(...).to_json_files(\"out/\", by=\"event_id\")\n",
    "# ‚Üí out/1234.json\n",
    "# ‚Üí out/1235.json\n",
    "```\n",
    "\n",
    "Name files by a field value:\n",
    "\n",
    "```python\n",
    "flow.to_json_files(\"output_by_player/\", by=\"player_name\")\n",
    "# ‚Üí output_by_player/Kevin De Bruyne.json\n",
    "# ‚Üí output_by_player/Erling Haaland.json\n",
    "# ‚Üí output_by_player/Bukayo Saka.json\n",
    "```\n",
    "\n",
    "üí° If multiple records share the same name, later ones may overwrite earlier ones.\n",
    "\n",
    "### `.to_jsonl()`: Save as JSON Lines\n",
    "\n",
    "Write all records into one file, one record per line.\n",
    "\n",
    "```python\n",
    "flow.to_jsonl(\"processed/events.jsonl\")\n",
    "# processed/events.jsonl containing:\n",
    "# {\"event_id\": 1, \"type_name\": \"Pass\", ...}\n",
    "# {\"event_id\": 2, \"type_name\": \"Shot\", ...}\n",
    "```\n",
    "\n",
    "üí° Great for reloading later with `.from_jsonl()`. \n",
    "\n",
    "### `.to_json_single()`: Save as JSON Array\n",
    "\n",
    "Write all records into a single `.json` file as an array.\n",
    "\n",
    "```python\n",
    "summary = Flow(...).group_by(...).summary(...)\n",
    "summary.to_json_single(\"summary.json\", indent=4)\n",
    "# summary.json containing:\n",
    "# [\n",
    "#   {\"event_id\": 1, \"type_name\": \"Pass\", ...},\n",
    "#   {\"event_id\": 2, \"type_name\": \"Shot\", ...}\n",
    "# ]\n",
    "```\n",
    "\n",
    "üí° Collects all records into memory before writing.\n",
    "\n",
    "## `.to_pandas()`: Convert to a Pandas DataFrame\n",
    "\n",
    "Handy when moving into pandas for further analysis or export.\n",
    "\n",
    "```python\n",
    "df = Flow(...).filter(...).select(...).to_pandas()\n",
    "df.to_csv(\"cleaned.csv\", index=False)\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Source Format        | Use Method              | Streaming? | Notes                        |\n",
    "| -------------------- | ----------------------- | ---------- | ---------------------------- |\n",
    "| Python dict/list/gen | `.from_records()`       | ‚úÖ          | Very flexible starting point |\n",
    "| JSON Lines file      | `.from_jsonl()`         | ‚úÖ          | Best for large datasets      |\n",
    "| Single JSON file     | `.from_file()`          | ‚ùå          | Loads entire file            |\n",
    "| Folder of JSON files | `.from_folder()`        | ‚úÖ          | Handles nested lists/records |\n",
    "| Glob pattern         | `.from_glob()`          | ‚úÖ          | Flexible file matching       |\n",
    "| StatsBomb GitHub     | `.statsbomb.from_...()` | ‚úÖ          | Download open data directly  |\n",
    "\n",
    "`Flow` makes it easy to load, process, and save JSON-centric data - helping you focus on analysis, not file wrangling.\n",
    "\n",
    "## What‚Äôs Next?\n",
    "\n",
    "Next, we‚Äôll look at how to inspect your data stream and understand its contents.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
