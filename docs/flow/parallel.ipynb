{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fede8f2",
   "metadata": {},
   "source": [
    "# Processing Multiple Files in Parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10642325",
   "metadata": {},
   "source": [
    "Often, your data isn't in a single monolithic file but spread across multiple files - perhaps one file per match, per player, or per day. Processing these files sequentially can be time-consuming. `Flow` provides a utility function, `folder_flow`, designed to process a directory of data files in parallel, applying your `Flow` transformations efficiently.\n",
    "\n",
    "## Core Idea:\n",
    "\n",
    "`folder_flow` works by:\n",
    "\n",
    "- Identifying all relevant data files (e.g., .json or .jsonl) in a specified input folder.\n",
    "- For each file, it spawns a separate process (up to a specified number of parallel jobs).\n",
    "- In each process:\n",
    "    - The data file is loaded into a `Flow`.\n",
    "    - A user-provided function (`flow_fn`) containing `Flow` pipeline logic is applied to this `Flow`.\n",
    "- Optionally, the processed results from each file can be:\n",
    "    - Written to a corresponding new file in an output folder.\n",
    "    - Collected and merged into a single Flow.\n",
    "\n",
    "Optionally, if results are merged, a final \"reduce\" function (`reduce_fn`) can be applied to this combined `Flow`.\n",
    "\n",
    "## Using folder_flow\n",
    "\n",
    "Here's the typical signature:\n",
    "\n",
    "```python\n",
    "from penaltyblog.matchflow import folder_flow\n",
    "\n",
    "folder_flow(\n",
    "    input_folder: Union[str, Path],\n",
    "    flow_fn: Callable[[\"Flow\"], \"Flow\"],\n",
    "    output_folder: Optional[Union[str, Path]] = None,\n",
    "    reduce_fn: Optional[Callable[[\"Flow\"], \"Flow\"]] = None,\n",
    "    n_jobs: Optional[int] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    file_exts: tuple[str, ...] = (\".json\", \".jsonl\"),\n",
    ") -> Optional[\"Flow\"]:\n",
    "```\n",
    "\n",
    "## Parameters:\n",
    "\n",
    "- input_folder: Path to the directory containing your data files.\n",
    "- flow_fn: Your processing logic. This is a Python function that:\n",
    "    - Must accept a Flow object (containing data from one input file) as its first argument.\n",
    "    - Must return a Flow object representing the processed data for that file.\n",
    "    - Crucially, `flow_fn` must be \"pickleable\" (meaning Python's pickle module can serialize it). This usually means it should be a named function. Complex closures or instance methods can sometimes cause issues (this is an in-built limitation of to Python rather than `Flow`). `folder_flow` will try to check this and raise an error if it's not pickleable.\n",
    "- output_folder (optional): If provided, the processed results from each input file will be saved as a new file (with the same name as the input file) in this directory. The format (`.json` or `.jsonl`) will match the input. If an output_folder is specified, `folder_flow` returns None.\n",
    "- reduce_fn (optional): If output_folder is not specified, the results from all processed files are collected and merged into a single `Flow`. This reduce_fn can then be applied to this combined `Flow` for final aggregations or transformations.\n",
    "    - Like `flow_fn`, it must accept a `Flow` and return a `Flow`, and be pickleable.\n",
    "- n_jobs (optional): The number of parallel worker processes to use. Defaults to the number of CPU cores on your machine. Set to 1 for sequential processing (useful for debugging).\n",
    "- encoding (optional): File encoding to use for reading and writing. Defaults to \"utf-8\".\n",
    "- file_exts (optional): A tuple of file extensions (e.g., (\".json\", \".jsonl\")) to identify which files in input_folder should be processed. Defaults to (\".json\", \".jsonl\").\n",
    "\n",
    "\n",
    "## Return Value:\n",
    "\n",
    "- If output_folder is specified: Returns None.\n",
    "- If output_folder is not specified: Returns a `Flow` object containing the combined (and optionally reduced) results from all processed files.\n",
    "\n",
    "## Example: Processing Multiple Match Event Files\n",
    "\n",
    "Let's say you have a folder `match_data/` containing several JSON event files (e.g., match1.json, match2.json, ...). You want to extract all shots, add a field indicating if the shot was a goal, and then:\n",
    "\n",
    "- Save each processed match's shots to a new file.\n",
    "- Alternatively, combine all shots from all matches and get a final count.\n",
    "\n",
    "### Define your flow_fn:\n",
    "\n",
    "This function will operate on the `Flow` created from a single match file.\n",
    "\n",
    "```python\n",
    "from penaltyblog.matchflow import folder_flow\n",
    "\n",
    "# Make sure this function is defined at the top level of your script or in an importable module.\n",
    "def extract_and_mark_shots(flow: Flow) -> Flow:\n",
    "    \"\"\"Processes a single match's events: filters for shots and marks goals.\"\"\"\n",
    "    return (\n",
    "        flow\n",
    "        .filter(lambda r: r.get(\"type_name\") == \"Shot\") \n",
    "        .assign(is_goal=lambda r: r.get(\"shot_outcome\") == \"Goal\") \n",
    "        .select(\"match_id\", \"player_name\", \"shot_xg\", \"is_goal\", \"period\", \"timestamp\")\n",
    "    )\n",
    "```\n",
    "\n",
    "### Using `folder_flow` to write individual processed files:\n",
    "\n",
    "```python\n",
    "input_dir = \"path/to/your/match_data/\"\n",
    "output_dir = \"path/to/your/processed_shots_per_match/\"\n",
    "\n",
    "# This will process all .json files in input_dir using extract_and_mark_shots,\n",
    "# save each result to output_dir, and run using all available CPU cores.\n",
    "folder_flow(\n",
    "    input_folder=input_dir,\n",
    "    flow_fn=extract_and_mark_shots,\n",
    "    output_folder=output_dir,\n",
    "    file_exts=(\".json\",) # Assuming files are .json\n",
    ")\n",
    "print(f\"Processed shot files saved to {output_dir}\")\n",
    "\n",
    "After this runs, output_dir will contain files like match1.json, match2.json, etc., each containing only the processed shot data for that match.\n",
    "```\n",
    "\n",
    "### Using `folder_flow` to combine results and apply a reduce_fn:\n",
    "\n",
    "Now, let's get a combined `Flow` of all shots from all matches and then count the total number of goals.\n",
    "\n",
    "```python\n",
    "def count_total_goals(all_shots_flow: Flow) -> Flow:\n",
    "    \"\"\"Takes a flow of all shots and returns a summary flow with total goals.\"\"\"\n",
    "    return all_shots_flow.filter(lambda r: r.get(\"is_goal\") is True) \\\n",
    "                         .summary(total_goals_scored=\"count\")\n",
    "\n",
    "input_dir = \"path/to/your/match_data/\"\n",
    "\n",
    "# No output_folder means results are merged\n",
    "all_processed_shots_flow = folder_flow(\n",
    "    input_folder=input_dir,\n",
    "    flow_fn=extract_and_mark_shots, # Same flow_fn as before\n",
    "    reduce_fn=count_total_goals,    # Apply our new reduce function\n",
    "    file_exts=(\".json\",)\n",
    ")\n",
    "\n",
    "if all_processed_shots_flow:\n",
    "    summary_data = all_processed_shots_flow.collect()\n",
    "    # summary_data would be like: [{\"total_goals_scored\": X}]\n",
    "    print(f\"Summary from all matches: {summary_data}\")\n",
    "else:\n",
    "    print(\"folder_flow did not return a Flow (should not happen if output_folder is None and no errors).\")\n",
    "```\n",
    "\n",
    "## Important Considerations for Parallel Processing:\n",
    "\n",
    "- Pickleability: As mentioned, `flow_fn` and `reduce_fn` must be pickleable. This is a requirement of Python's multiprocessing library. If you get errors related to pickling, ensure your functions are defined at the top level of a module or are simple enough not to rely on unpickleable closures.\n",
    "- No Shared State (Almost): Each worker process gets its own copy of the `flow_fn` and operates on its assigned file independently. Avoid trying to modify global variables or shared state directly from within `flow_fn` across processes, as it may not work as expected. The design of `folder_flow` (returning data or writing to distinct files) handles the results.\n",
    "- Resource Intensive `flow_fn`: If your `flow_fn` itself is very memory-intensive (e.g., it materializes a large part of a single file's `Flow` internally), running many such jobs in parallel could still strain system memory.\n",
    "- Debugging: Parallel processing can sometimes make debugging harder. If you encounter issues, try running `folder_flow` with `n_jobs=1`. This will run the processing sequentially in the main process, making stack traces easier to understand.\n",
    "- File Loading: `folder_flow` attempts to detect whether to load files as JSON Lines (.jsonl) or standard JSON (.json) based on the file extension.\n",
    "\n",
    "`folder_flow` provides a powerful way to scale your `Flow`-based analyses across many files by leveraging multi-core processors. By defining a clear processing function for a single file (`flow_fn`) and optionally a reduction function (`reduce_fn`), you can significantly speed up your workflows for common batch processing tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7067f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
