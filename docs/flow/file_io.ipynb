{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9a4616",
   "metadata": {},
   "source": [
    "# Working with Files: Input & Output\n",
    "\n",
    "A key part of any data processing workflow is reading data from various sources and writing out your results. `Flow` provides several convenient methods for handling common file formats, especially JSON-based ones typical in sports analytics.\n",
    "\n",
    "## Loading Data into Flow\n",
    "\n",
    "`Flow` offers class methods (which you call like `Flow.from_something(...)`) to create a `Flow` instance directly from data sources.\n",
    "\n",
    "### From In-Memory Python Objects (`.from_records()`)\n",
    "\n",
    "If you already have your data as a Python list of dictionaries (or just a single dictionary, or any iterable of dictionaries), you can easily create a `Flow`:\n",
    "\n",
    "```python\n",
    "# List of dictionaries\n",
    "my_data = [\n",
    "    {\"id\": 1, \"value\": \"A\"},\n",
    "    {\"id\": 2, \"value\": \"B\"}\n",
    "]\n",
    "flow_from_list = Flow.from_records(my_data)\n",
    "\n",
    "# Single dictionary\n",
    "single_record = {\"id\": 3, \"value\": \"C\"}\n",
    "flow_from_dict = Flow.from_records(single_record) # Creates a Flow with one record\n",
    "\n",
    "# From a generator function\n",
    "def my_generator():\n",
    "    for i in range(3):\n",
    "        yield {\"id\": i, \"generated_value\": i*10}\n",
    "\n",
    "flow_from_gen = Flow.from_records(my_generator())\n",
    "\n",
    "# You can also use the Flow constructor directly for iterables:\n",
    "flow_from_constructor = Flow(my_data)\n",
    "flow_from_constructor_gen = Flow(my_generator())\n",
    "```\n",
    "\n",
    "`.from_records()` is flexible and is often the starting point if your data is already in Python.\n",
    "\n",
    "### From JSON Lines Files (`.from_jsonl()`)\n",
    "\n",
    "JSON Lines (`.jsonl`) is a convenient format where each line in the file is a separate, valid JSON object. This format is excellent for streaming large datasets.\n",
    "\n",
    "```python\n",
    "# Assuming 'match_events.jsonl' exists and each line is a JSON event dictionary\n",
    "events_flow = Flow.from_jsonl(\"path/to/your/match_events.jsonl\")\n",
    "\n",
    "# You can also specify encoding if it's not UTF-8\n",
    "events_flow_custom_encoding = Flow.from_jsonl(\"data.jsonl\", encoding=\"latin-1\")\n",
    "\n",
    "# Now you can chain operations:\n",
    "shots_flow = events_flow.filter(lambda r: r.get(\"type_name\") == \"Shot\")\n",
    "shots_data = shots_flow.collect()\n",
    "```\n",
    "\n",
    "`Flow` will read the file line by line, parsing each line as a JSON record and yielding it into the stream. This is memory-efficient as the whole file isn't loaded at once.\n",
    "\n",
    "### From a Single JSON File (`.from_file()`)\n",
    "\n",
    "If your data is in a single JSON file that contains either a JSON array (a list of objects) or a single JSON object, you can use \n",
    "`.from_file()`:\n",
    "\n",
    "```python\n",
    "# Case 1: File contains a JSON array: [{\"id\":1,...}, {\"id\":2,...}]\n",
    "flow_from_array_file = Flow.from_file(\"path/to/data_array.json\")\n",
    "\n",
    "# Case 2: File contains a single JSON object: {\"match_id\":123, \"data\":{...}}\n",
    "# This will result in a Flow with a single record.\n",
    "flow_from_object_file = Flow.from_file(\"path/to/single_object.json\")\n",
    "```\n",
    "\n",
    "Note: `.from_file()` reads the entire file content into memory to parse the JSON structure, so it's best for files that comfortably fit in memory. For very large arrays of JSON objects, `.from_jsonl()` is preferred if you can use that format.\n",
    "\n",
    "### From a Folder of JSON Files (`.from_folder()`)\n",
    "\n",
    "If you have multiple JSON files in a single folder, and each file contains either a JSON array of records or a single JSON record, \n",
    "`.from_folder()` can load them all into a single `Flow`.\n",
    "\n",
    "```python\n",
    "# Assuming 'data_folder/' contains 'file1.json', 'file2.json', etc.\n",
    "# Each .json file can be a list of records or a single record.\n",
    "combined_flow = Flow.from_folder(\"path/to/your/data_folder\")\n",
    "\n",
    "# Non-JSON files in the folder are skipped.\n",
    "# The records from all JSON files are streamed together.\n",
    "```\n",
    "\n",
    "This method iterates through the files in the folder, reads each one, and yields its records.\n",
    "\n",
    "### From a Glob Pattern (`.from_glob()`)\n",
    "\n",
    "For more flexible file matching, including searching in subdirectories, you can use `.from_glob()`:\n",
    "\n",
    "```python\n",
    "# Load all JSON files in 'data_folder' and its subfolders\n",
    "all_json_flow = Flow.from_glob(\"path/to/your/data_folder/**/*.json\")\n",
    "\n",
    "# Load all event files from a competition season\n",
    "season_events_flow = Flow.from_glob(\"competitions/super_league/season_2023/events_*.json\")\n",
    "```\n",
    "\n",
    "`.from_glob()` uses Python's glob module to find matching file paths and then processes each file similar to `.from_folder().`\n",
    "\n",
    "### Special Loaders: StatsBomb Open Data (`.statsbomb.from_github_file()`)\n",
    "\n",
    "`Flow` includes a convenient helper to directly load StatsBomb open data files (events, matches, lineups) from their GitHub repository:\n",
    "\n",
    "```python\n",
    "# Load event data for StatsBomb match_id 7580\n",
    "match_id = 266516\n",
    "events_flow = Flow.statsbomb.from_github_file(match_id=match_id, type=\"events\")\n",
    "\n",
    "# Load lineup data for the same match\n",
    "lineups_flow = Flow.statsbomb.from_github_file(match_id=match_id, type=\"lineups\")\n",
    "\n",
    "# Process as usual\n",
    "shots = events_flow.filter(lambda r: r.get(\"type_name\") == \"Shot\").collect()\n",
    "```\n",
    "\n",
    "This method handles fetching the data from the URL and parsing the JSON.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6605503",
   "metadata": {},
   "source": [
    "## Saving Data from `Flow`\n",
    "\n",
    "Once you've processed your data, `Flow` provides methods to save the results. These methods typically consume the `Flow` (materialize it) as they write out the data and return the `Flow` instance itself, allowing for potential further chaining (though often saving is the last step).\n",
    "\n",
    "### To Individual JSON Files (`.to_json_files()`)\n",
    "\n",
    "This method writes each record in your Flow to a separate JSON file within a specified folder.\n",
    "\n",
    "```python\n",
    "shots_flow = events_flow.filter(lambda r: r.get(\"type_name\") == \"Shot\")\n",
    "\n",
    "# Create a folder 'output_shots' and save each shot event as a separate JSON file.\n",
    "# Files will be named 'record_1.json', 'record_2.json', etc. by default.\n",
    "shots_flow.to_json_files(\"output_shots/\")\n",
    "\n",
    "# You can also name files based on a field in the record:\n",
    "processed_flow.to_json_files(\"output_by_player/\", by=\"player_name\")\n",
    "# This would create files like 'Kevin_De_Bruyne.json', 'Erling_Haaland.json'.\n",
    "# Note: Duplicate names from the 'by' field will overwrite previous files.\n",
    "```\n",
    "\n",
    "The output folder will be created if it doesn't exist.\n",
    "\n",
    "### To a Single JSON Lines File (`.to_jsonl()`)\n",
    "\n",
    "Writes all records in the `Flow` to a single `.jsonl` file, with each record as a JSON string on a new line. This is often a good format for later reloading or for use with other stream-processing tools.\n",
    "\n",
    "```python\n",
    "processed_flow = events_flow.filter(...).assign(...)\n",
    "processed_flow.to_jsonl(\"path/to/output/processed_events.jsonl\")\n",
    "```\n",
    "\n",
    "### To a Single JSON File (as an array) (`.to_json_single()`)\n",
    "\n",
    "Saves all records from the `Flow` into a single JSON file, formatted as a JSON array (a list of objects).\n",
    "\n",
    "```python\n",
    "summary_flow = events_flow.group_by(\"team_name\").aggregate(total_shots=\"count\")\n",
    "summary_flow.to_json_single(\"path/to/output/team_summary.json\", indent=4) # indent for pretty printing\n",
    "```\n",
    "\n",
    "This method will collect all records into a list in memory before writing, so be mindful of memory usage for very large `Flows`.\n",
    "\n",
    "### To a Pandas DataFrame (`.to_pandas()`)\n",
    "\n",
    "A very common step is to convert your processed `Flow` into a pandas DataFrame for more advanced analysis, visualization, or to save in other formats (like CSV) via pandas.\n",
    "\n",
    "```python\n",
    "final_flow = events_flow.filter(...).select(...)\n",
    "df = final_flow.to_pandas()\n",
    "\n",
    "# Now you can use pandas capabilities:\n",
    "print(df.head())\n",
    "df.to_csv(\"output_data.csv\", index=False)\n",
    "```\n",
    "\n",
    "This also collects all records into memory to construct the DataFrame.\n",
    "\n",
    "Effectively loading your source data and saving your valuable results is critical. `Flow` aims to make these common IO tasks straightforward, especially for JSON-centric workflows. Next, we'll explore more advanced operations for manipulating and combining streams.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
