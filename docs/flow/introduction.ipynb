{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe42299",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Flow: a lazy, streaming data pipeline\n",
    "\n",
    "In football analytics, much of the data - events, matches, player tracking - is delivered as nested JSON records. These files can be large, noisy, and full of nested structure, but we often want to perform familiar operations: filtering, selecting fields, computing aggregates, or joining on player or match IDs.\n",
    "\n",
    "In many tasks, you want to run a **lightweight data pipeline** over streams of JSON/dict records without loading everything into memory.\n",
    "\n",
    "That’s where **Flow** comes in.\n",
    "\n",
    "## Understanding How `Flow` Works\n",
    "\n",
    "Think of a `Flow` as a smart, efficient assembly line for your data records. You start with a stream of raw data (like a list of game events from a JSON file), and then you tell `Flow` what steps to perform on each record as it moves along the line.\n",
    "\n",
    "### Planning Your Work (Lazy Evaluation & Chaining)\n",
    "\n",
    "When you use methods like `.filter(...)`, `.select(...),` or `.assign(...)` on a `Flow`, you're not immediately processing all your data. Instead, you're building a plan or a recipe. Each step you add creates a new `Flow` object that remembers all the previous steps plus the new one you just added. This is called \"chaining.\"\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# Let's say raw_events is a list of dictionaries from a JSON file\n",
    "# This line just sets up the starting point\n",
    "data_plan = Flow(raw_events)\n",
    "\n",
    "# This adds a \"filter\" step to the plan. No data is processed yet.\n",
    "filtered_plan = data_plan.filter(lambda event: event.get(\"type_name\") == \"Shot\")\n",
    "\n",
    "# This adds a \"select\" step. Still just a plan!\n",
    "final_plan = filtered_plan.select(\"player_name\", \"shot_outcome_name\")\n",
    "```\n",
    "\n",
    "At this stage, `final_plan` knows it needs to: 1) get raw_events, 2) filter for shots, 3) select player and outcome. But no actual filtering or selecting has happened. This is \"lazy evaluation\" – work is delayed until absolutely necessary.\n",
    "\n",
    "### Doing the Work (Materialization)\n",
    "\n",
    "The actual data processing – running records through your planned assembly line – happens only when you ask for the final results. This is often called \"materialization.\"\n",
    "\n",
    "Common ways to get results:\n",
    "\n",
    "- `.collect()`: Turns the final planned output into a Python list of dictionaries.\n",
    "- `.to_pandas()`: Converts the output into a pandas DataFrame.\n",
    "- `.to_jsonl(\"output.jsonl\")`: Writes each resulting record to a file.\n",
    "- Looping: `for record in final_plan: print(record)`\n",
    "\n",
    "**Example (continuing from above):**\n",
    "\n",
    "```python\n",
    "shot_data_list = final_plan.collect()\n",
    "# NOW the work happens:\n",
    "# 1. Records from raw_events are read one by one.\n",
    "# 2. Each record is checked if it's a \"Shot\".\n",
    "# 3. For shots, only \"player_name\" and \"shot_outcome_name\" are kept.\n",
    "# 4. These processed records are collected into shot_data_list.\n",
    "```\n",
    "\n",
    "###  How Flow Works: Lazy, One-Way by Default (but You Can Cache)\n",
    "\n",
    "`Flow` is designed for lazy, memory-efficient processing of potentially large datasets. It does not load or transform data until absolutely necessary (e.g., when you call `.collect()` or `.to_pandas()`).\n",
    "\n",
    "### Lazy and One-Way by Default\n",
    "\n",
    "By default, a `Flow` consumes its underlying data as it processes it — much like a generator. This means:\n",
    "\n",
    "- Most operations (`filter()`, `.assign()`, `.select()`) just build the pipeline without executing it.\n",
    "- When you finally collect or iterate the flow, it runs the full pipeline from scratch.\n",
    "\n",
    "```python\n",
    "flow = Flow(data)\n",
    "filtered = flow.filter(...)\n",
    "# nothing happens yet\n",
    "result = filtered.collect()  # Now it runs everything\n",
    "```\n",
    "\n",
    "### Behind the Scenes: `tee()` for Safety\n",
    "\n",
    "To minimize surprises, most operations like `.collect()`, `.__iter__()`, `.first()`, etc. internally use `itertools.tee()`. This splits the internal stream so:\n",
    "\n",
    "- You can call `.collect()` multiple times without losing data.\n",
    "- You can inspect the first record and still iterate the rest.\n",
    "\n",
    "```python\n",
    "flow = Flow(data)\n",
    "print(flow.first())      # peeks safely\n",
    "print(flow.collect())    # still has all data\n",
    "```\n",
    "\n",
    "But this safety comes at a slight cost: the more you inspect a flow, the more data is potentially buffered in memory.\n",
    "\n",
    "### If You Want a “Save Point”: Use `.cache()`\n",
    "\n",
    "If you plan to re-use a flow many times and don’t want to re-run or re-buffer:\n",
    "\n",
    "```python\n",
    "cached_flow = flow.cache()\n",
    "```\n",
    "\n",
    "- This materializes and stores all records in memory.\n",
    "- Subsequent operations are fast and repeatable.\n",
    "- Great for debugging or branching off multiple analyses.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use `Flow(data).filter(...).collect()` for one-time pipelines.\n",
    "- Use `.cache()` if you plan to re-use the flow or need stable results.\n",
    "- Avoid reusing non-cached flows in multiple places unless you understand the buffering behavior.\n",
    "- If in doubt, re-create the Flow from your source data.\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "The lazy streaming model used by `Flow` offers a set of practical advantages:\n",
    "\n",
    "- Scales to large files: Many football datasets (like event or tracking data) are too big to load fully into memory. `Flow` lets you filter, transform, and summarize data record-by-record without blowing your RAM.\n",
    "- Fast startup: Since no data is loaded until needed, your pipeline can be constructed instantly — even for gigabyte-scale files.\n",
    "- Modular and composable: Each step is clean and isolated. You can build up pipelines gradually and reuse common steps without side effects.\n",
    "- Debuggable and inspectable: You can safely peek at `flow.first()`, `.head()`, or call `.collect()` repeatedly thanks to internal buffering, without breaking the stream.\n",
    "- Easy to branch: Want to try a few different filters or summaries on the same base data? Use `.cache()` once and explore all you want.\n",
    "- Familiar feel: Flow is conceptually similar to chaining operations in pandas or SQL - but with streaming, nested JSON, and performance in mind.\n",
    "\n",
    "This model keeps your analysis efficient, testable, and expressive - especially valuable when wrangling real-world football data that is often messy and nested.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
